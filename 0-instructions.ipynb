{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Natural Language Generation\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In February 2019 Natrual Language Generation (NLG) made headlines as OpenAI, Elon Musk's AI research company, released research showing that they could convincingly generate meaningful articles from simple prompts, and [it really freaked some people out](https://www.theguardian.com/commentisfree/2019/feb/15/ai-write-robot-openai-gpt2-elon-musk). To understand this in a bit more depth, [OpenAI's original blog post](https://blog.openai.com/better-language-models/) provides interesting examples.\n",
    "\n",
    "\n",
    "![gpt2](images/gpt2.gif)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "As with every AI breakthrough, it will take some time to understand what the limitations of this new technology is. As a general rule of thumb, new AI technologies tend to be more limited in the domain of problems they can solve than the public initially expects. Seeing this technology in action makes it feel like large swathes of journalism might be automated tomorrow, but it's not clear. The best way to understand if that it true is to look at the underlying technology, and the best way to understand that technology is to experiment with it.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "This python environment will allow you to experiment with two different text-to-text NLG techniques. Text-to-text differs to rules-based techniques, which take a spreadsheet of structured data and apply rule written by a human programmer to connect those data points with natural language (coherent text). Text-to-text techniques take a body of text that you provide, learn the patterns of language that exist in that text, and then use those learnings to generate new, but similar text. This is particularly useful if you want to generate text from a short prompt.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Traditionally these techniques have been good at mimicking style, but failed to make coherent, logical sense over the entirety of the generated text. This is because text-to-text heavily relies on a technology called machine learning, which has machines write their own rules using data, instead of relying on the expertise of human programmers to explicitly write the rules themselves. Having machines learn writing style is easier than having them learn the significance of facts, which is why rules-based techniques have generally won out for automated reporting tasks, but that may soon be changing.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Preparation\n",
    "\n",
    "<br><br>\n",
    "\n",
    "1. Find your own text that you want you \"AI writer\" to learn from. Two pages of text, or 2000-4000 should be enough. Any longer than 4000 and expect to wait longer for your first technique to learn!\n",
    "2. Copy the text you want to use.\n",
    "3. In your Jupyter Notebook directory, where you found this file select `New` (in the top right hand corner) -> `Text File` and paste in your text.\n",
    "4. Rename your file by clicking on `untitled.txt` at the top and renaming it to something short but useful, like `potter.txt`, if maybe you've pasted in some Harry Potter text. <b>Make sure you keep the `.txt` extension</b>.\n",
    "5. Click `File` -> `Save`, and a little check mark should appear next to your file name at the top of the page.\n",
    "6. You've loaded in your text data. Go back in to the main directory (it should be the first tab you opened up).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Technique 1: Recurrent Neural Networks\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are the technology at the heart of the OpenAI example above. By constructing machine learning algorithms that model the scale of connections seen in brains, complex behaviour can be learned, but doing so takes a lot of data, computer power, and research.\n",
    "\n",
    "We're not going to be using OpenAI's code, instead we're going to use some open source code that gives us access to the same basic technique they used, just not at the same scale of complexity.\n",
    "\n",
    "The open source library we will be using is called [textgenrnn](https://github.com/minimaxir/textgenrnn) which was found by using the developer and data scientists favourite code discovery tool - Google. RNNs applied to text generally learn and write on a character-by-character basis, because handling 27 possible characters is a much smaller set of buidling blocks than the 20,000 words in an average adults vocabulary. This also means that RNNS are entirely capable of making up new words if given the freedom.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "1. Open `1-text-generation-rnn.ipynb`\n",
    "2. Read the instructions and run the code in the first two gray boxes. You should already have generated some text. This model has come pretrained on language from discussion forums on the popular developer news site news.ycombinator.com, which might explain a bit of profanity here and there.\n",
    "3. Experiment with different prefixes, and then train the model on your own text and check the output. The longer the text, the longer it takes to train.\n",
    "4. Note the failures and successes of your output. Re-run the code a few times to try and get a sense of what patterns your RNN is picking up on.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "To learn more about RNNs, skim through [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/), a seminal blog post on the subject by Andrej Kaparthy, one of the leading researchers in the field.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Technique 2: Markov Chains\n",
    "\n",
    "<br><br>\n",
    "\n",
    "What if you don't want to go through the tedium of writing character by character? That's not really how we write, we remember words don't we?\n",
    "\n",
    "To look at characters and not words, we need a technique that looks back over fixed windows of characters, ie... words. To do this we're going to actually use a simpler technique than RNNs called a Markov Chain, which is a general mathematical technique for remembering patterns of events. In this case our events are going to be words. For the more mathematically curious of you, [this](http://setosa.io/ev/markov-chains/) is a visual explanation of how Markov Chains work. There is even [debate as to whether markov chains are a type of RNN](https://news.ycombinator.com/item?id=11187545), but that is far more depth than we need to go in to here.\n",
    "\n",
    "All we really need to know is that they're simpler than RNNs, work on a word-by-word level, and so require less computation power while hopefully giving us more reliable results.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "1. Open `2-text-generation-markov.ipynb`\n",
    "2. Read the instructions and run the code. Re-run the code with your text a few times.\n",
    "3. Compare the output to that of your RNN. Which made more sense? Which did a better job of capturing the writing style of the original writer?\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Summary\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Is your \"AI author\" going to take anyone's job any time soon? Perhaps not, but the small successes that you outputs did have may give us a sign of what's to come.\n",
    "\n",
    "If an RNN can learn sequences of characters that form words, and sequences of words that form coherent phrases, the next level of abstraction is using phrases to form concepts and concepts to form narratives. It is these latter stages of abstraction that cutting edge research, such as the OpenAI work above, is now active in.\n",
    "\n",
    "That we are now seeing output of research that is able to build coherent narratives from short prompts suggest that the main constraints are scale and time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
